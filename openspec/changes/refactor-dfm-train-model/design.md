# DFMè®­ç»ƒæ¨¡å—å®Œå…¨é‡æ„è®¾è®¡æ–‡æ¡£

## Context

å½“å‰HTFAé¡¹ç›®ä¸­å­˜åœ¨ä¸¤ä¸ªDFMè®­ç»ƒå®ç°ï¼š
- `train_model/`ï¼šåŸå§‹å®ç°ï¼ˆ15,049è¡Œï¼Œ24ä¸ªæ–‡ä»¶ï¼‰
- `train_ref/`ï¼šé‡æ„ç‰ˆæœ¬ï¼ˆ2,673è¡Œï¼Œå®é™…åŠŸèƒ½ä»…è¦†ç›–40%ï¼‰

ç»è¿‡æ·±åº¦ä»£ç åˆ†æå‘ç°ï¼Œtrain_refçš„æ ¸å¿ƒç®—æ³•å±‚å·²å®Œæˆï¼Œä½†å…³é”®åŠŸèƒ½æ¨¡å—ç¼ºå¤±ï¼š
- å˜é‡é€‰æ‹©å±‚ï¼ˆselection/ï¼‰- 1,200è¡Œç¼ºå¤±
- è®­ç»ƒåè°ƒå±‚ï¼ˆtraining/trainer.py, pipeline.pyï¼‰- 3,500è¡Œç¼ºå¤±
- åˆ†æè¾“å‡ºå±‚ï¼ˆanalysis/ï¼‰- 3,300è¡Œç¼ºå¤±
- é¢„è®¡ç®—å¼•æ“ï¼ˆoptimization/precompute.pyï¼‰- 800è¡Œç¼ºå¤±

**é‡‡ç”¨å®Œå…¨é‡æ„**ï¼šé‡æ–°å®ç°æ‰€æœ‰ç¼ºå¤±æ¨¡å—ï¼Œå½»åº•è§£å†³æ¶æ„é—®é¢˜ã€‚

**train_modelå¤„ç†ç­–ç•¥**ï¼š
- âš ï¸ åœ¨æ‰€æœ‰é‡æ„å·¥ä½œã€ä¸€è‡´æ€§éªŒè¯ã€ç”¨æˆ·æµ‹è¯•å®Œæˆå‰ï¼Œ**ä¸å¾—åˆ é™¤**train_modelæ¨¡å—
- ğŸ“– train_modelä»£ç ä»…ä½œä¸º**å‚è€ƒ**ï¼Œä¸å¾—è¢«è°ƒç”¨æˆ–ä¿®æ”¹
- âœ… åªæœ‰åœ¨ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œåï¼Œæ‰åœ¨Phase 9åˆ é™¤train_model

**åˆ†æ”¯ç®¡ç†ç­–ç•¥**ï¼š
- ğŸŒ¿ æ‰€æœ‰é‡æ„å·¥ä½œåœ¨`feature/refactor-train-model`åˆ†æ”¯è¿›è¡Œ
- ğŸš« é‡æ„æœŸé—´ä¸å¾—åˆå¹¶åˆ°mainåˆ†æ”¯
- âœ… åªæœ‰åœ¨Phase 9ï¼ˆæ¸…ç†ä¸åˆå¹¶ï¼‰å®Œå…¨å®Œæˆåï¼Œæ‰åˆå¹¶åˆ°main

## Goals / Non-Goals

### Goals

1. **åŠŸèƒ½ç­‰ä»·**ï¼šé‡æ–°å®ç°ä¸train_modelå®Œå…¨ç›¸åŒçš„åŠŸèƒ½
2. **æ•°å€¼ä¸€è‡´**ï¼šç›¸åŒè¾“å…¥ä¸‹ï¼Œè¾“å‡ºç»“æœæ•°å€¼è¯¯å·® < 1e-6
3. **æ¶æ„æ¸…æ™°**ï¼šåˆ†å±‚æ˜ç¡®ï¼ŒèŒè´£å•ä¸€ï¼Œæ˜“äºç»´æŠ¤
4. **ä»£ç è´¨é‡**ï¼šéµå¾ªKISSã€DRYã€YAGNIã€SOCã€SRPåŸåˆ™
5. **é«˜æµ‹è¯•è¦†ç›–**ï¼šæ ¸å¿ƒç®—æ³•å±‚ > 90%ï¼Œå…¶ä»–å±‚ > 80%
6. **å½»åº•æ¶ˆé™¤æŠ€æœ¯å€º**ï¼šä¸ä¿ç•™ä»»ä½•é—ç•™ä»£ç 

### Non-Goals

1. **ä¿ç•™å…¼å®¹**ï¼šä¸æä¾›train_modelä¸train_refçš„å…¼å®¹å±‚æˆ–å›é€€æœºåˆ¶
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šä¸å¼•å…¥è¶…å‡ºtrain_modelçš„æ–°ä¼˜åŒ–ï¼ˆå¦‚GPUåŠ é€Ÿï¼‰
3. **åŠŸèƒ½æ‰©å±•**ï¼šä¸æ·»åŠ train_modelä¸æ”¯æŒçš„æ–°åŠŸèƒ½ï¼ˆå¦‚å¢é‡è®­ç»ƒï¼‰
4. **æ›´æ”¹æ•°æ®æ ¼å¼**ï¼šä¿æŒè¾“å…¥è¾“å‡ºæ ¼å¼ä¸train_modelä¸€è‡´

## Decisions

### 1. åˆ†å±‚æ¶æ„ä¿æŒä¸å˜

é‡‡ç”¨train_refå·²ç¡®ç«‹çš„åˆ†å±‚è®¾è®¡ï¼š

```
train_ref/
â”œâ”€â”€ core/           # æ ¸å¿ƒç®—æ³•ï¼ˆå¡å°”æ›¼æ»¤æ³¢ã€DFMæ¨¡å‹ã€EMä¼°è®¡ï¼‰
â”œâ”€â”€ evaluation/     # è¯„ä¼°å±‚ï¼ˆè¯„ä¼°å™¨ã€æŒ‡æ ‡ã€éªŒè¯å™¨ï¼‰
â”œâ”€â”€ selection/      # å˜é‡é€‰æ‹©å±‚ï¼ˆåå‘é€‰æ‹©å™¨ã€é€‰æ‹©å¼•æ“ï¼‰
â”œâ”€â”€ optimization/   # ä¼˜åŒ–å±‚ï¼ˆç¼“å­˜ã€é¢„è®¡ç®—ï¼‰
â”œâ”€â”€ training/       # è®­ç»ƒåè°ƒå±‚ï¼ˆè®­ç»ƒå™¨ã€æµç¨‹ç¼–æ’ã€é…ç½®ï¼‰
â”œâ”€â”€ analysis/       # åˆ†æè¾“å‡ºå±‚ï¼ˆæŠ¥å‘Šç”Ÿæˆã€å¯è§†åŒ–ï¼‰
â”œâ”€â”€ utils/          # å·¥å…·å±‚ï¼ˆæ•°æ®å·¥å…·ã€æ—¥å¿—ã€å¯é‡ç°æ€§ï¼‰
â””â”€â”€ facade.py       # ç»Ÿä¸€APIå…¥å£
```

**ç†ç”±**ï¼š
- æ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼Œé™ä½è€¦åˆ
- å•å‘ä¾èµ–å…³ç³»ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
- å·²éªŒè¯æœ‰æ•ˆï¼ˆæ ¸å¿ƒå±‚å·²ç¨³å®šè¿è¡Œï¼‰

### 2. å˜é‡é€‰æ‹©å®ç°ç­–ç•¥

**å†³ç­–**ï¼šé‡æ–°å®ç°åå‘é€æ­¥å˜é‡é€‰æ‹©ç®—æ³•

**æ ¸å¿ƒç®—æ³•**ï¼š
```python
# selection/backward_selector.py
class BackwardSelector:
    def __init__(self, evaluator: ModelEvaluator, precompute_engine: PrecomputeEngine):
        self.evaluator = evaluator
        self.precompute = precompute_engine

    def select(self,
               data: pd.DataFrame,
               target_col: str,
               initial_variables: List[str],
               criterion: str = 'rmse',
               progress_callback: Optional[Callable] = None) -> SelectionResult:
        """åå‘é€æ­¥å˜é‡é€‰æ‹©

        ç®—æ³•æµç¨‹ï¼š
        1. ä»å…¨éƒ¨å˜é‡å¼€å§‹
        2. é€ä¸ªå°è¯•å‰”é™¤æ¯ä¸ªå˜é‡
        3. è¯„ä¼°å‰”é™¤åçš„æ¨¡å‹æ€§èƒ½
        4. é€‰æ‹©æ€§èƒ½æå‡æœ€å¤§çš„å˜é‡å‰”é™¤
        5. é‡å¤ç›´åˆ°æ— æ³•æå‡
        """
        current_vars = initial_variables.copy()
        selection_history = []

        # é¢„è®¡ç®—åæ–¹å·®çŸ©é˜µåŠ é€Ÿè¯„ä¼°
        precomputed_ctx = self.precompute.compute(data[current_vars + [target_col]])

        while len(current_vars) > 1:
            best_score = -np.inf
            best_var_to_remove = None

            # å°è¯•å‰”é™¤æ¯ä¸ªå˜é‡
            for var in current_vars:
                candidate_vars = [v for v in current_vars if v != var]
                score = self._evaluate_subset(
                    data, candidate_vars, target_col, criterion, precomputed_ctx
                )

                if score > best_score:
                    best_score = score
                    best_var_to_remove = var

            # æ£€æŸ¥æ˜¯å¦åœæ­¢
            baseline_score = self._evaluate_subset(
                data, current_vars, target_col, criterion, precomputed_ctx
            )

            if best_score <= baseline_score:
                break

            # å‰”é™¤å˜é‡å¹¶è®°å½•
            current_vars.remove(best_var_to_remove)
            selection_history.append({
                'removed_variable': best_var_to_remove,
                'score': best_score,
                'remaining_vars': current_vars.copy()
            })

            if progress_callback:
                progress_callback(f"[SELECTION] å‰”é™¤: {best_var_to_remove}, "
                                f"å‰©ä½™: {len(current_vars)}, å¾—åˆ†: {best_score:.4f}")

        return SelectionResult(
            selected_variables=current_vars,
            selection_history=selection_history,
            final_score=best_score
        )
```

**ç†ç”±**ï¼š
- å®Œå…¨æ§åˆ¶ç®—æ³•å®ç°ï¼Œæ˜“äºè°ƒè¯•
- ä¾èµ–æ³¨å…¥ä¾¿äºæµ‹è¯•
- ä½¿ç”¨é¢„è®¡ç®—å¼•æ“ä¼˜åŒ–æ€§èƒ½

**å®ç°è¦ç‚¹**ï¼š
1. å‚è€ƒtrain_model/variable_selection.pyçš„æ ¸å¿ƒé€»è¾‘
2. æ”¯æŒRMSEå’ŒHit RateåŒç›®æ ‡ä¼˜åŒ–
3. å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼ˆ>85%è¦†ç›–ç‡ï¼‰

### 3. è®­ç»ƒæµç¨‹ç¼–æ’

**å†³ç­–**ï¼šä½¿ç”¨Pipelineè®¾è®¡æ¨¡å¼å®ç°ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹

```python
# training/pipeline.py
class TrainingPipeline:
    def __init__(self, selection_engine, factor_model, evaluator):
        self.selection_engine = selection_engine
        self.factor_model = factor_model
        self.evaluator = evaluator

    def run(self, config: TrainingConfig, progress_callback=None) -> TrainingResult:
        """å®Œæ•´ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹"""
        # é˜¶æ®µ1ï¼šå˜é‡é€‰æ‹©
        stage1_result = self._run_stage1(config, progress_callback)

        # é˜¶æ®µ2ï¼šå› å­æ•°é€‰æ‹©
        stage2_result = self._run_stage2(config, stage1_result, progress_callback)

        # æœ€ç»ˆè®­ç»ƒ
        final_result = self._run_final_training(config, stage2_result, progress_callback)

        return self._merge_results(stage1_result, stage2_result, final_result)

    def _run_stage1(self, config, progress_callback):
        """é˜¶æ®µ1ï¼šå˜é‡é€‰æ‹©ï¼ˆå›ºå®šk=å—æ•°ï¼‰"""
        if not config.selection.enable:
            return Stage1Result(selected_variables=config.data.selected_indicators)

        data = load_data(config.data.data_path)
        result = self.selection_engine.run(
            selection_method=config.selection.method,
            data=data,
            config=config,
            progress_callback=progress_callback
        )
        return Stage1Result(selected_variables=result.selected_variables,
                          selection_history=result.selection_history)

    def _run_stage2(self, config, stage1_result, progress_callback):
        """é˜¶æ®µ2ï¼šå› å­æ•°é€‰æ‹©ï¼ˆPCA/Elbow/Fixedï¼‰"""
        data = load_data(config.data.data_path)[stage1_result.selected_variables]

        if config.model.factor_selection_method == 'fixed':
            return Stage2Result(k_factors=config.model.k_factors)

        elif config.model.factor_selection_method == 'cumulative':
            pca_analysis = self._perform_pca_analysis(data)
            k = self._select_k_by_cumulative_variance(
                pca_analysis.eigenvalues,
                threshold=config.model.pca_threshold
            )
            return Stage2Result(k_factors=k, pca_analysis=pca_analysis)

        elif config.model.factor_selection_method == 'elbow':
            pca_analysis = self._perform_pca_analysis(data)
            k = self._select_k_by_elbow(
                pca_analysis.eigenvalues,
                threshold=config.model.elbow_threshold
            )
            return Stage2Result(k_factors=k, pca_analysis=pca_analysis)

    def _run_final_training(self, config, stage2_result, progress_callback):
        """æœ€ç»ˆæ¨¡å‹è®­ç»ƒ"""
        model = self.factor_model.fit(
            data=data,
            k_factors=stage2_result.k_factors,
            max_iter=config.model.max_iterations,
            progress_callback=progress_callback
        )

        # è¯„ä¼°
        metrics = self.evaluator.evaluate(model, data, config)

        return FinalResult(model=model, metrics=metrics)
```

**ç†ç”±**ï¼š
- æ¸…æ™°çš„ä¸¤é˜¶æ®µæµç¨‹åˆ†ç¦»
- æ¯ä¸ªé˜¶æ®µå¯ç‹¬ç«‹æµ‹è¯•
- æ”¯æŒæœªæ¥æ‰©å±•ï¼ˆå¦‚æ·»åŠ é˜¶æ®µ3ï¼‰

### 4. ç»“æœåˆ†æå±‚å®ç°

**å†³ç­–**ï¼šå®Œæ•´é‡æ–°å®ç°åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–åŠŸèƒ½

```python
# analysis/reporter.py
class AnalysisReporter:
    def generate_full_report(self, model, data, config) -> AnalysisReport:
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        return AnalysisReport(
            pca_analysis=self.generate_pca_report(model),
            contribution_analysis=self.generate_contribution_report(model, data),
            r2_analysis=self.generate_r2_report(model, data),
            factor_loadings=self.calculate_factor_loadings(model)
        )

    def generate_pca_report(self, model) -> pd.DataFrame:
        """PCAæ–¹å·®è´¡çŒ®åˆ†æ"""
        eigenvalues = model.get_eigenvalues()
        variance_ratios = eigenvalues / np.sum(eigenvalues)
        cumulative_variance = np.cumsum(variance_ratios)

        return pd.DataFrame({
            'å› å­ç´¢å¼•': range(1, len(eigenvalues) + 1),
            'ç‰¹å¾å€¼': eigenvalues,
            'æ–¹å·®è´¡çŒ®ç‡': variance_ratios,
            'ç´¯ç§¯æ–¹å·®è´¡çŒ®ç‡': cumulative_variance
        })

    def generate_contribution_report(self, model, data) -> pd.DataFrame:
        """å› å­è´¡çŒ®åº¦åˆ†è§£ï¼ˆæŒ‰æŒ‡æ ‡ã€æŒ‰è¡Œä¸šï¼‰"""
        factors = model.get_factors()
        loadings = model.get_loadings()

        # è®¡ç®—è´¡çŒ®åº¦ = å› å­è½½è· * å› å­å€¼ * æ ‡å‡†å·®
        contributions = {}
        for i, var in enumerate(data.columns):
            contrib = loadings[i, :] * factors * data[var].std()
            contributions[var] = contrib.sum(axis=0)

        return pd.DataFrame(contributions)

    def generate_r2_report(self, model, data) -> pd.DataFrame:
        """ä¸ªä½“RÂ²å’Œè¡Œä¸šRÂ²è®¡ç®—"""
        # å®ç°ä¸ªä½“å˜é‡RÂ²è®¡ç®—
        # å®ç°è¡Œä¸šèšåˆRÂ²è®¡ç®—
        pass
```

**ç†ç”±**ï¼š
- ä¸è®­ç»ƒé€»è¾‘è§£è€¦
- ä¾¿äºæ‰©å±•æ–°çš„åˆ†æç±»å‹
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆExcelã€CSVã€JSONï¼‰

### 5. æ•°å€¼ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

**å†³ç­–**ï¼šé‡‡ç”¨ä¸¥æ ¼çš„å¯¹æ¯”æµ‹è¯•æ¡†æ¶

```python
# tests/consistency/test_end_to_end.py
class TestNumericConsistency:
    def test_parameter_estimation(self):
        """éªŒè¯EMç®—æ³•å‚æ•°ä¼°è®¡ç»“æœä¸€è‡´"""
        # åŠ è½½ç›¸åŒæ•°æ®å’Œé…ç½®
        data = load_test_data()
        config = create_test_config()

        # è¿è¡Œtrain_refï¼ˆåœ¨åˆ é™¤train_modelå‰ä¿ç•™å¯¹æ¯”ï¼‰
        from dashboard.DFM.train_ref import DFMTrainer
        new_result = DFMTrainer(config).train()

        # åŠ è½½train_modelçš„åŸºå‡†ç»“æœ
        baseline_result = load_baseline_results('test_case_1.pkl')

        # éªŒè¯å‚æ•°çŸ©é˜µ
        assert np.linalg.norm(new_result.params.A - baseline_result.params.A) < 1e-6
        assert np.linalg.norm(new_result.params.Q - baseline_result.params.Q) < 1e-6
        assert np.linalg.norm(new_result.params.H - baseline_result.params.H) < 1e-6
        assert np.linalg.norm(new_result.params.R - baseline_result.params.R) < 1e-6

    def test_forecast_consistency(self):
        """éªŒè¯é¢„æµ‹ç»“æœä¸€è‡´"""
        # å¯¹æ¯”æ¯ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹å€¼
        for t in range(len(new_result.forecast_oos)):
            assert abs(new_result.forecast_oos[t] - baseline_result.forecast_oos[t]) < 1e-6

    def test_metrics_consistency(self):
        """éªŒè¯è¯„ä¼°æŒ‡æ ‡ä¸€è‡´"""
        assert abs(new_result.metrics.rmse_oos - baseline_result.metrics.rmse_oos) < 1e-4
        assert abs(new_result.metrics.hit_rate_oos - baseline_result.metrics.hit_rate_oos) < 0.01
```

**å®æ–½æ­¥éª¤**ï¼š
1. åœ¨åˆ é™¤train_modelå‰ï¼Œè¿è¡Œtrain_modelå¹¶ä¿å­˜æ‰€æœ‰ç»“æœä½œä¸ºbaseline
2. å®ç°train_refæ–°æ¨¡å—æ—¶ï¼ŒæŒç»­å¯¹æ¯”baselineéªŒè¯æ•°å€¼ä¸€è‡´æ€§
3. æ‰€æœ‰å¯¹æ¯”æµ‹è¯•é€šè¿‡åï¼Œæ‰å¯åˆ é™¤train_model

### 6. UIå±‚ç›´æ¥åˆ‡æ¢

**å†³ç­–**ï¼šç›´æ¥ä¿®æ”¹UIä½¿ç”¨train_refï¼Œä¸ä¿ç•™ç‰ˆæœ¬åˆ‡æ¢

```python
# dashboard/ui/pages/dfm/model_training_page.py

# æ—§ä»£ç ï¼ˆåˆ é™¤ï¼‰
# from dashboard.DFM.train_model.tune_dfm import run_tuning
# results = run_tuning(...)

# æ–°ä»£ç 
from dashboard.DFM.train_ref import DFMTrainer, TrainingConfig

# æ„å»ºé…ç½®
config = TrainingConfig(
    data=DataConfig(
        data_path=selected_file,
        target_variable=target_var,
        ...
    ),
    model=ModelConfig(
        k_factors=k_factors,
        max_iter=max_iterations,
        ...
    ),
    selection=SelectionConfig(
        enable=enable_selection,
        method='backward',
        criterion='rmse'
    )
)

# è®­ç»ƒ
trainer = DFMTrainer(config)
results = trainer.train(progress_callback=lambda msg: update_progress(msg))
```

**ç†ç”±**ï¼š
- ç®€åŒ–ä»£ç ï¼Œæ— éœ€ç»´æŠ¤ç‰ˆæœ¬åˆ‡æ¢é€»è¾‘
- ç”¨æˆ·ä½“éªŒå®Œå…¨ä¸€è‡´ï¼ˆæ¥å£ä¿æŒä¸å˜ï¼‰
- å½»åº•ç§»é™¤é—ç•™ä»£ç 

## Risks / Trade-offs

### é£é™©1ï¼šæ•°å€¼ç²¾åº¦å·®å¼‚

**æè¿°**ï¼šé‡æ–°å®ç°ç®—æ³•å¯èƒ½å¼•å…¥å¾®å°æ•°å€¼è¯¯å·®

**ç¼“è§£**ï¼š
- å‚è€ƒtrain_modelå®ç°ï¼Œç¡®ä¿ç®—æ³•é€»è¾‘æ­£ç¡®
- å»ºç«‹å…¨é¢çš„å¯¹æ¯”æµ‹è¯•ï¼ˆè¯¯å·® < 1e-6ï¼‰
- ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­å’Œæ•°å€¼åº“ç‰ˆæœ¬

**æ¥å—åº¦**ï¼šRMSEå·®å¼‚ < 1e-4å¯æ¥å—

### é£é™©2ï¼šå¼€å‘å‘¨æœŸé•¿

**æè¿°**ï¼š22å‘¨å¼€å‘å‘¨æœŸå¯èƒ½å½±å“å…¶ä»–é¡¹ç›®

**ç¼“è§£**ï¼š
- åˆ†é˜¶æ®µå¼€å‘ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯äº¤ä»˜æˆæœ
- å¹¶è¡Œå¼€å‘å¤šä¸ªæ¨¡å—ï¼ˆé€‰æ‹©å±‚ã€åˆ†æå±‚ï¼‰
- æŒç»­é›†æˆå’Œæµ‹è¯•ï¼ŒåŠæ—©å‘ç°é—®é¢˜

### é£é™©3ï¼šå¤æ‚ç®—æ³•å®ç°éš¾åº¦é«˜

**æè¿°**ï¼šå˜é‡é€‰æ‹©ã€ç»“æœåˆ†æç­‰ç®—æ³•å¤æ‚åº¦é«˜

**ç¼“è§£**ï¼š
- æ·±å…¥ç ”ç©¶train_modelå®ç°
- åˆ†æ­¥å®ç°ï¼Œå…ˆå®ç°æ ¸å¿ƒé€»è¾‘å†ä¼˜åŒ–
- é«˜æµ‹è¯•è¦†ç›–ç‡ï¼ˆ>90%ï¼‰

### å…³é”®æƒè¡¡

1. **å¼€å‘æ—¶é—´ vs é•¿æœŸç»´æŠ¤**ï¼š22å‘¨æ¢å–60%ç»´æŠ¤æˆæœ¬é™ä½
2. **é£é™© vs æ”¶ç›Š**ï¼šé«˜é£é™©æ¢å–å½»åº•è§£å†³æŠ€æœ¯å€ºåŠ¡
3. **å®Œç¾ vs å®ç”¨**ï¼šä¸å¼•å…¥æ–°ä¼˜åŒ–ï¼Œä¸“æ³¨åŠŸèƒ½ç­‰ä»·

## Migration Plan

### Phase 1ï¼šå‡†å¤‡é˜¶æ®µï¼ˆ1å‘¨ï¼‰

1. è¿è¡Œtrain_modelç”Ÿæˆæ‰€æœ‰baselineç»“æœ
2. ä¿å­˜baselineåˆ°tests/consistency/baseline/
3. åˆ›å»ºfeatureåˆ†æ”¯è¿›è¡Œå¼€å‘

### Phase 2ï¼šæ ¸å¿ƒå¼€å‘ï¼ˆ16å‘¨ï¼‰

**Week 1-3**ï¼šå˜é‡é€‰æ‹©å±‚
- å®ç°BackwardSelector
- å®ç°SelectionEngine
- å•å…ƒæµ‹è¯•ï¼ˆè¦†ç›–ç‡ > 85%ï¼‰

**Week 4-7**ï¼šè®­ç»ƒåè°ƒå±‚
- å®ç°TrainingPipeline
- å®ç°DFMTrainer
- è¡¥å……é…ç½®ç±»
- å•å…ƒæµ‹è¯•ï¼ˆè¦†ç›–ç‡ > 80%ï¼‰

**Week 8-11**ï¼šåˆ†æè¾“å‡ºå±‚
- å®ç°AnalysisReporter
- å®ç°ResultVisualizer
- å®ç°åˆ†æå·¥å…·å‡½æ•°
- å•å…ƒæµ‹è¯•ï¼ˆè¦†ç›–ç‡ > 80%ï¼‰

**Week 12-13**ï¼šä¼˜åŒ–å±‚
- å®ç°PrecomputeEngine
- å®ç°ä¼˜åŒ–è¯„ä¼°å™¨
- æ€§èƒ½æµ‹è¯•

**Week 14-16**ï¼šé›†æˆæµ‹è¯•
- ç«¯åˆ°ç«¯å¯¹æ¯”æµ‹è¯•
- æ•°å€¼ä¸€è‡´æ€§éªŒè¯
- æ€§èƒ½åŸºå‡†æµ‹è¯•

### Phase 3ï¼šUIè¿ç§»ï¼ˆ2å‘¨ï¼‰

1. ä¿®æ”¹model_training_page.py
2. ä¿®æ”¹ç›¸å…³ç»„ä»¶
3. å…¨é¢UIæµ‹è¯•

### Phase 4ï¼šä¸Šçº¿ï¼ˆ2å‘¨ï¼‰

1. ä»£ç å®¡æŸ¥
2. éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
3. ç”¨æˆ·éªŒæ”¶æµ‹è¯•
4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

### Phase 5ï¼šæ¸…ç†ä¸åˆå¹¶ï¼ˆ1å‘¨ï¼‰

**å‰ç½®æ¡ä»¶**ï¼š
- Phase 4æ‰€æœ‰æµ‹è¯•å’Œéƒ¨ç½²å·²å®Œæˆ
- ç”¨æˆ·éªŒæ”¶æµ‹è¯•é€šè¿‡
- ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œè‡³å°‘1å‘¨

**ä»»åŠ¡**ï¼š
1. åˆ é™¤train_modelç›®å½•ï¼ˆæ»¡è¶³å‰ç½®æ¡ä»¶åï¼‰
2. æ›´æ–°æ–‡æ¡£ï¼ˆCLAUDE.mdï¼‰
3. å‘å¸ƒå˜æ›´æ—¥å¿—
4. åˆå¹¶featureåˆ†æ”¯åˆ°main
5. åˆ›å»ºå‘å¸ƒæ ‡ç­¾v2.0.0-train-ref

**æ€»è®¡**ï¼š22å‘¨

## Open Questions

1. **æ˜¯å¦éœ€è¦ä¿ç•™train_modelçš„æ€§èƒ½åˆ†ææ¨¡å—**ï¼Ÿ
   - **å†³ç­–**ï¼šæš‚ä¸å®ç°ï¼Œä½œä¸ºå¯é€‰åŠŸèƒ½åç»­æ·»åŠ 

2. **æ˜¯å¦éœ€è¦æ”¯æŒå¹¶è¡Œè®­ç»ƒ**ï¼Ÿ
   - **å†³ç­–**ï¼šä¿ç•™ä¸²è¡Œå®ç°ï¼Œæ€§èƒ½å¯æ¥å—

3. **æµ‹è¯•è¦†ç›–ç‡æœ€ä½æ ‡å‡†**ï¼Ÿ
   - **å†³ç­–**ï¼šæ ¸å¿ƒç®—æ³•å±‚ > 90%ï¼Œå…¶ä»–å±‚ > 80%
