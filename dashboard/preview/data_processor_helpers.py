# -*- coding: utf-8 -*-
"""
数据处理辅助函数模块
将data_loader.py中的巨型函数拆分为可复用的小函数
"""
import pandas as pd
import numpy as np
import warnings
import io
from typing import Dict, List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


def read_indicator_mapping(
    excel_file_handler,
    sheet_name: str = '指标体系',
    file_buffer=None
) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, str]]:
    """
    读取指标体系映射（行业、单位、类型）

    Args:
        excel_file_handler: pandas ExcelFile对象
        sheet_name: 指标体系sheet名称
        file_buffer: 文件缓冲区（用于BytesIO seek操作）

    Returns:
        Tuple[Dict, Dict, Dict]: (行业映射, 单位映射, 类型映射)
    """
    from dashboard.preview.data_loader import normalize_string

    industry_map = {}
    unit_map = {}
    type_map = {}

    try:
        if file_buffer and isinstance(file_buffer, io.BytesIO):
            file_buffer.seek(0)

        # 检查列名
        temp_df = pd.read_excel(excel_file_handler, sheet_name=sheet_name, nrows=0)
        available_cols = temp_df.columns.tolist()

        # 准备要读取的列
        cols_to_read = []
        col_mapping = {
            '指标名称': ['指标名称'],
            '行业': ['行业'],
            '单位': ['单位'],
            '类型': ['类型']
        }

        actual_col_names = {}
        for target_col, possible_names in col_mapping.items():
            for name in possible_names:
                if name in available_cols:
                    cols_to_read.append(name)
                    actual_col_names[target_col] = name
                    break

        if not actual_col_names.get('指标名称'):
            warnings.warn(f"Sheet '{sheet_name}' found, but no indicator name column found.")
            return industry_map, unit_map, type_map

        df_mapping = pd.read_excel(
            excel_file_handler,
            sheet_name=sheet_name,
            usecols=cols_to_read
        )

        # 统一列名
        rename_dict = {v: k for k, v in actual_col_names.items()}
        df_mapping.rename(columns=rename_dict, inplace=True)

        # 标准化指标名称
        df_mapping['指标名称'] = df_mapping['指标名称'].apply(normalize_string)

        # 创建行业映射
        if '行业' in df_mapping.columns:
            df_industry = df_mapping.dropna(subset=['指标名称', '行业'])
            industry_map = pd.Series(
                df_industry['行业'].values,
                index=df_industry['指标名称']
            ).to_dict()
            logger.debug(f"成功读取 {len(industry_map)} 个行业映射")

        # 创建单位映射
        if '单位' in df_mapping.columns:
            df_unit = df_mapping.dropna(subset=['指标名称', '单位'])
            df_unit = df_unit[df_unit['单位'].str.strip() != '']
            unit_map = pd.Series(
                df_unit['单位'].values,
                index=df_unit['指标名称']
            ).to_dict()
            logger.debug(f"成功读取 {len(unit_map)} 个单位映射")

        # 创建类型映射
        if '类型' in df_mapping.columns:
            df_type = df_mapping.dropna(subset=['指标名称', '类型'])
            df_type = df_type[df_type['类型'].str.strip() != '']
            type_map = pd.Series(
                df_type['类型'].values,
                index=df_type['指标名称']
            ).to_dict()
            logger.debug(f"成功读取 {len(type_map)} 个类型映射")

    except KeyError as ke:
        warnings.warn(f"Sheet '{sheet_name}' columns not found: {ke}")
    except Exception as e:
        warnings.warn(f"Error reading mapping sheet '{sheet_name}': {e}")

    return industry_map, unit_map, type_map


def determine_data_type(sheet_name: str) -> Optional[str]:
    """
    根据sheet名称判断数据类型（从配置导入，避免硬编码）

    Args:
        sheet_name: Excel sheet名称

    Returns:
        Optional[str]: 数据类型 ('weekly'/'monthly'/'daily'/'ten_day'/'yearly' 或 None)
    """
    from dashboard.preview.config import CHINESE_TO_ENGLISH_FREQ

    # 使用配置字典，避免硬编码
    for chinese_name, english_name in CHINESE_TO_ENGLISH_FREQ.items():
        if chinese_name in sheet_name:
            return english_name

    return None


def process_date_column(
    df: pd.DataFrame,
    sheet_name: str
) -> Optional[pd.DataFrame]:
    """
    处理日期列：转换为DatetimeIndex，去重，排序

    Args:
        df: 输入DataFrame
        sheet_name: Sheet名称（用于日志）

    Returns:
        Optional[pd.DataFrame]: 处理后的DataFrame，失败返回None
    """
    date_col_name = df.columns[0]
    logger.debug(f"使用 '{date_col_name}' 作为日期列")

    try:
        df[date_col_name] = pd.to_datetime(df[date_col_name], errors='coerce')
        original_rows = len(df)
        df = df.dropna(subset=[date_col_name])

        if len(df) < original_rows:
            warnings.warn(
                f"Sheet '{sheet_name}': Removed {original_rows - len(df)} rows with invalid dates."
            )

        if df.empty:
            warnings.warn(f"Sheet '{sheet_name}' became empty after removing invalid dates.")
            return None

        df.set_index(date_col_name, inplace=True)
        df.sort_index(inplace=True)

        if not df.index.is_unique:
            warnings.warn(f"Sheet '{sheet_name}': Found duplicate dates, keeping last.")
            df = df[~df.index.duplicated(keep='last')]

        return df

    except Exception as e:
        warnings.warn(f"Error processing date column for sheet '{sheet_name}': {e}")
        return None


def convert_to_numeric_and_clean(
    df: pd.DataFrame,
    sheet_name: str,
    threshold: float = 1e-9
) -> Optional[pd.DataFrame]:
    """
    转换为数值类型并清理零值和近零值（向量化优化版）

    Args:
        df: 输入DataFrame
        sheet_name: Sheet名称（用于日志）
        threshold: 近零值阈值

    Returns:
        Optional[pd.DataFrame]: 处理后的DataFrame，失败返回None
    """
    logger.debug(f"尝试对 {len(df.columns)} 列进行数值转换")

    # 向量化转换：一次性转换所有列
    df = df.apply(pd.to_numeric, errors='coerce')
    logger.debug("数值转换完成")

    numeric_cols = df.select_dtypes(include=np.number).columns

    if not numeric_cols.empty:
        logger.debug(f"对数值列应用近零阈值 ({threshold}) 并替换零值: {list(numeric_cols)}")

        # 向量化操作：一次性对所有数值列进行mask操作
        # 创建条件掩码（所有列的绝对值小于阈值或等于0）
        mask = (df[numeric_cols].abs() < threshold) | (df[numeric_cols] == 0)

        # 一次性应用mask到所有数值列
        df.loc[:, numeric_cols] = df[numeric_cols].where(~mask, np.nan)

        # 检查是否所有数据都变成NaN
        if df[numeric_cols].isnull().all().all():
            warnings.warn(f"Sheet '{sheet_name}' is all NaN after conversion and cleaning.")
            return None
    else:
        logger.warning("数值转换后未找到数值列")
        return None

    return df


def _process_periodic_frequency(
    df: pd.DataFrame,
    data_type: str,
    freq_param: str
) -> pd.DataFrame:
    """处理周期性频率数据的通用函数（周度、旬度）

    消除周度和旬度处理的重复代码

    Args:
        df: 输入DataFrame
        data_type: 数据类型 ('weekly' 或 'ten_day')
        freq_param: 借调逻辑参数 ('W-FRI' 或 'ten_day')

    Returns:
        pd.DataFrame: 处理后的DataFrame
    """
    from dashboard.preview.data_loader import standardize_timestamps, apply_borrowing_logic

    freq_name = '周度' if data_type == 'weekly' else '旬度'
    logger.debug(f"处理{freq_name}数据")

    # 去重
    if not df.index.is_unique:
        original_count = len(df)
        df = df[~df.index.duplicated(keep='first')]
        logger.debug(f"移除 {original_count - len(df)} 个重复时间戳")

    # 时间戳标准化
    df = standardize_timestamps(df, data_type)

    # 借调逻辑
    df = apply_borrowing_logic(df, freq_param)
    logger.debug(f"已应用{freq_name}数据借调逻辑")

    return df


def process_frequency_specific(
    df: pd.DataFrame,
    data_type: str,
    source_prefix: str,
    indicator_source_map: Dict[str, str]
) -> pd.DataFrame:
    """
    按频率进行特定处理（周度、旬度的时间戳标准化和借调）

    Args:
        df: 输入DataFrame
        data_type: 数据类型
        source_prefix: 数据源前缀
        indicator_source_map: 指标来源映射字典（会被修改）

    Returns:
        pd.DataFrame: 处理后的DataFrame
    """
    from dashboard.preview.data_loader import normalize_string

    # 月度数据特殊处理：工业增加值日期调整
    if data_type == "monthly":
        cols_to_adjust = [col for col in df.columns if "工业增加值" in col]
        if cols_to_adjust:
            logger.debug(f"调整'工业增加值'指标的日期索引 (找到 {len(cols_to_adjust)} 个)")
            if pd.api.types.is_datetime64_any_dtype(df.index):
                try:
                    df.index = df.index - pd.offsets.MonthEnd(1)
                    logger.debug(f"调整后的索引范围: {df.index.min()} 至 {df.index.max()}")
                except Exception as e:
                    warnings.warn(f"Failed to apply MonthEnd offset: {e}")

    # 周度数据处理（使用通用函数）
    elif data_type == "weekly":
        df = _process_periodic_frequency(df, "weekly", 'W-FRI')

    # 旬度数据处理（使用通用函数）
    elif data_type == "ten_day":
        df = _process_periodic_frequency(df, "ten_day", 'ten_day')

    # 更新指标来源映射并标准化列名
    for col in df.columns:
        map_key = normalize_string(col)
        if map_key not in indicator_source_map:
            indicator_source_map[map_key] = source_prefix

    df.columns = [normalize_string(col) for col in df.columns]

    return df


def process_single_sheet(
    excel_file_handler,
    sheet_name: str,
    file_name: str,
    source_name_base: str,
    indicator_source_map: Dict[str, str],
    file_buffer=None
) -> Optional[Tuple[str, pd.DataFrame]]:
    """处理单个Excel sheet

    Args:
        excel_file_handler: pandas ExcelFile对象
        sheet_name: sheet名称
        file_name: 文件名
        source_name_base: 源文件基础名称
        indicator_source_map: 指标来源映射字典
        file_buffer: 文件缓冲区

    Returns:
        Optional[Tuple[str, pd.DataFrame]]: (数据类型, 处理后的DataFrame)，失败返回None
    """
    logger.debug(f"分析sheet: '{sheet_name}'")

    # 跳过指标体系sheet
    if sheet_name == '指标体系':
        return None

    # 判断数据类型
    data_type = determine_data_type(sheet_name)
    if data_type is None:
        logger.debug(f"Sheet名称不包含数据类型标记，跳过")
        return None

    try:
        # Reset buffer position if reading from BytesIO
        if file_buffer and isinstance(file_buffer, io.BytesIO):
            file_buffer.seek(0)

        logger.debug(f"从sheet '{sheet_name}' 读取 '{data_type}' 数据")

        # 使用统一格式读取
        read_params = {
            'sheet_name': sheet_name,
            'header': 0,
            'dtype': object,
            'keep_default_na': False,
            'na_values': []
        }

        df_sheet = pd.read_excel(excel_file_handler, **read_params)

        if df_sheet.empty:
            warnings.warn(f"     Sheet '{sheet_name}' in '{file_name}' is empty. Skipping.")
            return None

        # 验证格式
        if len(df_sheet.columns) < 2:
            warnings.warn(f"     Sheet '{sheet_name}': 列数不足（需要至少2列：时间列+数据列）. Skipping.")
            return None

        logger.debug(f"成功读取 {len(df_sheet.columns)} 列: 第一列为时间列，其余{len(df_sheet.columns)-1}列为数据变量")

        # 处理日期列
        df_sheet = process_date_column(df_sheet, sheet_name)
        if df_sheet is None:
            return None

        # 数值转换和清理
        df_sheet = convert_to_numeric_and_clean(df_sheet, sheet_name)
        if df_sheet is None:
            return None

        # 频率特定处理
        source_identifier_prefix = f"{source_name_base}|{sheet_name}"
        df_processed = process_frequency_specific(
            df_sheet,
            data_type,
            source_identifier_prefix,
            indicator_source_map
        )

        return (data_type, df_processed)

    except Exception as e:
        warnings.warn(f"     Error processing sheet '{sheet_name}' in '{file_name}': {e}. Skipping sheet.")
        return None


def process_single_excel_file(
    file_input,
    indicator_industry_map: Dict[str, str],
    indicator_unit_map: Dict[str, str],
    indicator_type_map: Dict[str, str],
    read_mapping_sheet: bool = False
) -> Tuple[Dict[str, List[pd.DataFrame]], Dict[str, str], Dict[str, str], Dict[str, str], Dict[str, str]]:
    """处理单个Excel文件

    Args:
        file_input: 上传的文件对象
        indicator_industry_map: 行业映射字典（会被更新）
        indicator_unit_map: 单位映射字典（会被更新）
        indicator_type_map: 类型映射字典（会被更新）
        read_mapping_sheet: 是否读取指标体系映射

    Returns:
        Tuple: (数据框字典, 指标来源映射, 行业映射, 单位映射, 类型映射)
    """
    # 初始化结果容器
    all_dfs_by_type = {
        'weekly': [],
        'monthly': [],
        'daily': [],
        'ten_day': [],
        'yearly': []
    }
    indicator_source_map = {}

    # 处理文件输入 - 兼容多种文件对象类型
    try:
        # 优先使用Streamlit上传文件的getvalue()方法
        if hasattr(file_input, 'getvalue'):
            file_buffer = io.BytesIO(file_input.getvalue())
            file_name_for_display = getattr(file_input, 'name', 'uploaded_file.xlsx')
        # 否则尝试使用read()方法(普通文件对象)
        elif hasattr(file_input, 'read'):
            content = file_input.read()
            if isinstance(content, bytes):
                file_buffer = io.BytesIO(content)
            else:
                file_buffer = io.BytesIO(content.encode())
            file_input.seek(0)  # 重置文件指针
            file_name_for_display = getattr(file_input, 'name', 'uploaded_file.xlsx')
        else:
            warnings.warn(f"Unsupported file input type: {type(file_input)}")
            return all_dfs_by_type, indicator_source_map, indicator_industry_map, indicator_unit_map, indicator_type_map
    except Exception as e:
        warnings.warn(f"Failed to read file: {e}")
        return all_dfs_by_type, indicator_source_map, indicator_industry_map, indicator_unit_map, indicator_type_map
    source_name_base = file_name_for_display.rsplit('.', 1)[0] if '.' in file_name_for_display else file_name_for_display

    try:
        excel_file_handler = pd.ExcelFile(file_buffer)
        logger.info(f"处理文件: {file_name_for_display}")
        sheet_names = excel_file_handler.sheet_names
        logger.debug(f"找到sheets: {', '.join(sheet_names)}")

        # 读取指标体系（如果需要）
        if read_mapping_sheet and '指标体系' in sheet_names:
            logger.debug("尝试从'指标体系'sheet读取映射")
            industry_map_temp, unit_map_temp, type_map_temp = read_indicator_mapping(
                excel_file_handler,
                '指标体系',
                file_buffer
            )
            if industry_map_temp:
                indicator_industry_map.update(industry_map_temp)
            if unit_map_temp:
                indicator_unit_map.update(unit_map_temp)
            if type_map_temp:
                indicator_type_map.update(type_map_temp)

        # 处理每个sheet
        processed_sheets = 0
        for sheet_name in sheet_names:
            result = process_single_sheet(
                excel_file_handler,
                sheet_name,
                file_name_for_display,
                source_name_base,
                indicator_source_map,
                file_buffer
            )

            if result:
                data_type, df_processed = result
                all_dfs_by_type[data_type].append(df_processed)
                processed_sheets += 1

        if processed_sheets > 0:
            logger.info(f"文件 {file_name_for_display}: 已处理 {processed_sheets} 个sheets")
        else:
            warnings.warn(f"--- No valid data sheets found in file: {file_name_for_display} ---")

    except Exception as e:
        warnings.warn(f"General error processing file {file_name_for_display}: {e}")
    finally:
        if 'excel_file_handler' in locals():
            try:
                excel_file_handler.close()
            except Exception:
                pass

    return all_dfs_by_type, indicator_source_map, indicator_industry_map, indicator_unit_map, indicator_type_map


def merge_dataframes_by_type(
    all_dfs_dict: Dict[str, List[pd.DataFrame]]
) -> Dict[str, pd.DataFrame]:
    """
    按数据类型合并DataFrame列表（从配置导入，避免硬编码）

    Args:
        all_dfs_dict: 数据类型到DataFrame列表的字典

    Returns:
        Dict[str, pd.DataFrame]: 数据类型到合并后DataFrame的字典
    """
    from dashboard.preview.config import ENGLISH_TO_CHINESE_FREQ

    result = {}

    # 使用配置字典，避免硬编码
    for data_type, df_list in all_dfs_dict.items():
        type_name = ENGLISH_TO_CHINESE_FREQ.get(data_type, data_type)
        logger.debug(f"合并{type_name}数据")

        if df_list:
            merged_df = pd.concat(df_list, axis=1)
            merged_df = merged_df.loc[:, ~merged_df.columns.duplicated(keep='first')]
            merged_df.sort_index(inplace=True)
            result[data_type] = merged_df
            logger.info(f"最终{type_name}数据形状: {merged_df.shape}")
        else:
            result[data_type] = pd.DataFrame()
            logger.debug(f"未找到{type_name}数据")

    return result
